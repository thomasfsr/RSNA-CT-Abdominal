{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport random\nfrom PIL import Image\nimport pydicom\nfrom sklearn.model_selection import train_test_split\nimport nibabel as nib\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport pytorch_lightning as L\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchmetrics\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:49:48.691208Z","iopub.execute_input":"2023-11-06T23:49:48.691859Z","iopub.status.idle":"2023-11-06T23:50:02.575569Z","shell.execute_reply.started":"2023-11-06T23:49:48.691826Z","shell.execute_reply":"2023-11-06T23:50:02.574805Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"class Config:\n    SEED = 101\n    BATCH_SIZE = 16\n    MAX_EPOCHS = 5\n    LR=0.005\n\nconfig = Config()\nrandom.seed(config.SEED)\nnp.random.seed(config.SEED)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:50:02.577235Z","iopub.execute_input":"2023-11-06T23:50:02.577513Z","iopub.status.idle":"2023-11-06T23:50:02.582737Z","shell.execute_reply.started":"2023-11-06T23:50:02.577487Z","shell.execute_reply":"2023-11-06T23:50:02.581610Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"original = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/train.csv')\ntest_or = pd.read_parquet('/kaggle/input/rsna-2023-abdominal-trauma-detection/test_dicom_tags.parquet')","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:50:33.267747Z","iopub.execute_input":"2023-11-06T23:50:33.268316Z","iopub.status.idle":"2023-11-06T23:50:33.495236Z","shell.execute_reply.started":"2023-11-06T23:50:33.268285Z","shell.execute_reply":"2023-11-06T23:50:33.494433Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_injuried = original[original['any_injury']==1]\ninjuried = df_injuried['patient_id']","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:50:33.496657Z","iopub.execute_input":"2023-11-06T23:50:33.496962Z","iopub.status.idle":"2023-11-06T23:50:33.503510Z","shell.execute_reply.started":"2023-11-06T23:50:33.496936Z","shell.execute_reply":"2023-11-06T23:50:33.502708Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_not_injuried = original[original['any_injury']==0]\nnot_injuried = df_not_injuried['patient_id']","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:01:07.406188Z","iopub.execute_input":"2023-11-07T00:01:07.407026Z","iopub.status.idle":"2023-11-07T00:01:07.412446Z","shell.execute_reply.started":"2023-11-07T00:01:07.406992Z","shell.execute_reply":"2023-11-07T00:01:07.411538Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"len(injuried)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:01:09.204525Z","iopub.execute_input":"2023-11-07T00:01:09.205352Z","iopub.status.idle":"2023-11-07T00:01:09.211428Z","shell.execute_reply.started":"2023-11-07T00:01:09.205317Z","shell.execute_reply":"2023-11-07T00:01:09.210389Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"855"},"metadata":{}}]},{"cell_type":"code","source":"len(not_injuried)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:01:09.432280Z","iopub.execute_input":"2023-11-07T00:01:09.432656Z","iopub.status.idle":"2023-11-07T00:01:09.438632Z","shell.execute_reply.started":"2023-11-07T00:01:09.432626Z","shell.execute_reply":"2023-11-07T00:01:09.437602Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"2292"},"metadata":{}}]},{"cell_type":"code","source":"df_not_injuried_200 = df_not_injuried[:200]\nnot_injuried_200 = df_not_injuried_200['patient_id']","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:01:10.266790Z","iopub.execute_input":"2023-11-07T00:01:10.267515Z","iopub.status.idle":"2023-11-07T00:01:10.273426Z","shell.execute_reply.started":"2023-11-07T00:01:10.267479Z","shell.execute_reply":"2023-11-07T00:01:10.272312Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"len(df_not_injuried_200)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:01:16.994528Z","iopub.execute_input":"2023-11-07T00:01:16.995464Z","iopub.status.idle":"2023-11-07T00:01:17.001907Z","shell.execute_reply.started":"2023-11-07T00:01:16.995424Z","shell.execute_reply":"2023-11-07T00:01:17.000948Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"200"},"metadata":{}}]},{"cell_type":"code","source":"#Creation of the path of the DICOM images\nroot_dir = '/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images'\npatients = injuried\n\nserial_path = {'patient_id':[],\n               'serial_id':[],\n               'serial_path':[]\n              }\n\nserials = {'patient_path':[],\n           'patient': []\n          }\n# Populate serials['patient_path'] first\nfor patient_id in patients:\n    patient_path = os.path.join(root_dir, str(patient_id))\n    serials['patient_path'].append(patient_path)\n    serials['patient'].append(patient_id)\n    \nserial_list = list(serials.values())\npatient_paths, patient_ids = serial_list\n\n#Now, let's create entries in serial_path for each serial ID\nfor patient_path, patient_id in zip(patient_paths, patient_ids):\n    serial_ids = os.listdir(patient_path)\n    for serial_id in serial_ids:\n        serial_path['patient_id'].append(patient_id)\n        serial_path['serial_id'].append(serial_id)\n        serial_path['serial_path'].append(os.path.join(patient_path,serial_id))\n\ndf_serials_inj = pd.DataFrame(serial_path)\n\nimage_paths = {'patient_id':[],'serial_id':[],'serial_path':[],'image_path':[], 'image':[] }\n\npatient_path_dir = df_serials_inj['patient_id']\nserial_id_dir = df_serials_inj['serial_id']\nserial_path_dir = df_serials_inj['serial_path']\n\nfor patient_id, serial_id, serial_path in zip(patient_path_dir, serial_id_dir, serial_path_dir):\n    images = os.listdir(serial_path)\n    for image in images:\n        image_paths['patient_id'].append(patient_id)\n        image_paths['serial_id'].append(serial_id)\n        image_paths['image_path'].append(os.path.join(serial_path,image))\n        image_paths['serial_path'].append(serial_path)\n        image_paths['image'].append(image)\n        \nimage_paths_df = pd.DataFrame(image_paths)\n\nimage_paths_df['image'] = image_paths_df['image'].str.replace('.dcm', '').astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:04:21.022248Z","iopub.execute_input":"2023-11-07T00:04:21.022920Z","iopub.status.idle":"2023-11-07T00:04:23.942326Z","shell.execute_reply.started":"2023-11-07T00:04:21.022889Z","shell.execute_reply":"2023-11-07T00:04:23.941557Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#Creation of the path of the DICOM images\nroot_dir = '/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images'\npatients = not_injuried_200\n\nserial_path = {'patient_id':[],\n               'serial_id':[],\n               'serial_path':[]\n              }\n\nserials = {'patient_path':[],\n           'patient': []\n          }\n# Populate serials['patient_path'] first\nfor patient_id in patients:\n    patient_path = os.path.join(root_dir, str(patient_id))\n    serials['patient_path'].append(patient_path)\n    serials['patient'].append(patient_id)\n    \nserial_list = list(serials.values())\npatient_paths, patient_ids = serial_list\n\n#Now, let's create entries in serial_path for each serial ID\nfor patient_path, patient_id in zip(patient_paths, patient_ids):\n    serial_ids = os.listdir(patient_path)\n    for serial_id in serial_ids:\n        serial_path['patient_id'].append(patient_id)\n        serial_path['serial_id'].append(serial_id)\n        serial_path['serial_path'].append(os.path.join(patient_path,serial_id))\n\ndf_serials_n_inj = pd.DataFrame(serial_path)\n\nimage_paths = {'patient_id':[],'serial_id':[],'serial_path':[],'image_path':[], 'image':[] }\n\npatient_path_dir = df_serials_n_inj['patient_id']\nserial_id_dir = df_serials_n_inj['serial_id']\nserial_path_dir = df_serials_n_inj['serial_path']\n\nfor patient_id, serial_id, serial_path in zip(patient_path_dir, serial_id_dir, serial_path_dir):\n    images = os.listdir(serial_path)\n    for image in images:\n        image_paths['patient_id'].append(patient_id)\n        image_paths['serial_id'].append(serial_id)\n        image_paths['image_path'].append(os.path.join(serial_path,image))\n        image_paths['serial_path'].append(serial_path)\n        image_paths['image'].append(image)\n        \nimage_paths_df_not_inj = pd.DataFrame(image_paths)\n\nimage_paths_df_not_inj['image'] = image_paths_df_not_inj['image'].str.replace('.dcm', '').astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:04:58.385977Z","iopub.execute_input":"2023-11-07T00:04:58.386626Z","iopub.status.idle":"2023-11-07T00:04:59.036644Z","shell.execute_reply.started":"2023-11-07T00:04:58.386594Z","shell.execute_reply":"2023-11-07T00:04:59.035651Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"len(image_paths_df_not_inj)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:57:10.095884Z","iopub.execute_input":"2023-11-06T23:57:10.096256Z","iopub.status.idle":"2023-11-06T23:57:10.102410Z","shell.execute_reply.started":"2023-11-06T23:57:10.096220Z","shell.execute_reply":"2023-11-06T23:57:10.101491Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"89194"},"metadata":{}}]},{"cell_type":"code","source":"len(image_paths_df)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:57:21.001095Z","iopub.execute_input":"2023-11-06T23:57:21.001452Z","iopub.status.idle":"2023-11-06T23:57:21.008173Z","shell.execute_reply.started":"2023-11-06T23:57:21.001424Z","shell.execute_reply":"2023-11-06T23:57:21.007020Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"430154"},"metadata":{}}]},{"cell_type":"code","source":"image_paths_df_total = pd.concat([image_paths_df,image_paths_df_not_inj],axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:58:13.820525Z","iopub.execute_input":"2023-11-06T23:58:13.821118Z","iopub.status.idle":"2023-11-06T23:58:13.840181Z","shell.execute_reply.started":"2023-11-06T23:58:13.821088Z","shell.execute_reply":"2023-11-06T23:58:13.839141Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#Separate the serial_id with more than 50 images, less than 50 images and exactly 50 images\npidf = pd.DataFrame()\nlidf = pd.DataFrame()\nmidf = pd.DataFrame()\nnum_slices = 50\nfor serial, group in image_paths_df_total.groupby('serial_id'):\n    if len(group) == num_slices:\n        group_sorted = group.sort_values('image')\n        pidf = pd.concat([pidf,group_sorted],axis=0,ignore_index=True)\n    elif len(group) > num_slices:\n        group_sorted = group.sort_values('image')\n        midf = pd.concat([midf,group_sorted],axis=0,ignore_index=True)\n    else:\n        group_sorted = group.sort_values('image')\n        lidf = pd.concat([lidf,group_sorted],axis=0,ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:58:31.186215Z","iopub.execute_input":"2023-11-06T23:58:31.187012Z","iopub.status.idle":"2023-11-06T23:58:47.134235Z","shell.execute_reply.started":"2023-11-06T23:58:31.186983Z","shell.execute_reply":"2023-11-06T23:58:47.133366Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"new_more_df = pd.DataFrame()  \nfor serial_id, group in midf.groupby('serial_id'):\n    total_rows = group.shape[0]\n    step_size = max(total_rows // num_slices, 1)  \n    selected_rows = group.iloc[::step_size][:num_slices].reset_index(drop=True)\n    new_more_df = pd.concat([new_more_df, selected_rows], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:58:47.135669Z","iopub.execute_input":"2023-11-06T23:58:47.135964Z","iopub.status.idle":"2023-11-06T23:58:50.075570Z","shell.execute_reply.started":"2023-11-06T23:58:47.135939Z","shell.execute_reply":"2023-11-06T23:58:50.074579Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"new_less_df = pd.DataFrame()\nfor serial, group in lidf.groupby('serial_id'):\n    length = len(group)\n    intergers = num_slices//length\n    rest = num_slices%length\n    group_df = pd.concat([group] * intergers, axis=0)\n    if rest >0:\n        step_size = max(length // rest, 1)\n        selected_rows = group.iloc[::step_size][:rest]\n        inter_df = pd.concat([group_df,selected_rows], axis=0)\n    new_less_df = pd.concat([new_less_df,inter_df], axis=0)\n    new_less_df = new_less_df.sort_index()","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:58:50.077140Z","iopub.execute_input":"2023-11-06T23:58:50.077413Z","iopub.status.idle":"2023-11-06T23:58:50.100631Z","shell.execute_reply.started":"2023-11-06T23:58:50.077390Z","shell.execute_reply":"2023-11-06T23:58:50.099642Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df_inj_ninj = pd.concat([df_injuried,df_not_injuried_200],axis=0,ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:01:55.771589Z","iopub.execute_input":"2023-11-07T00:01:55.772124Z","iopub.status.idle":"2023-11-07T00:01:55.777250Z","shell.execute_reply.started":"2023-11-07T00:01:55.772087Z","shell.execute_reply":"2023-11-07T00:01:55.776292Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"total_dfs = pd.concat([pidf,new_less_df,new_more_df],axis=0,ignore_index=True)\ndf_total = pd.merge(total_dfs,df_inj_ninj, on='patient_id')","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:02:17.319118Z","iopub.execute_input":"2023-11-07T00:02:17.319508Z","iopub.status.idle":"2023-11-07T00:02:17.345355Z","shell.execute_reply.started":"2023-11-07T00:02:17.319475Z","shell.execute_reply":"2023-11-07T00:02:17.344544Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df_serials = pd.concat([df_serials_inj,df_serials_n_inj],axis=0,ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:05:34.522049Z","iopub.execute_input":"2023-11-07T00:05:34.522771Z","iopub.status.idle":"2023-11-07T00:05:34.527732Z","shell.execute_reply.started":"2023-11-07T00:05:34.522730Z","shell.execute_reply":"2023-11-07T00:05:34.526804Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_s = pd.merge(df_serials, df_injuried,on = 'patient_id')\nX = df_s['serial_id']\ny = df_s[['bowel_healthy',\n          'bowel_injury', \n          'extravasation_healthy', \n          'extravasation_injury',\n          #'kidney_healthy', \n          #'kidney_low', \n          #'kidney_high', \n          'liver_healthy',\n          #'liver_low', \n          #'liver_high', \n          'spleen_healthy',\n          'spleen_low',\n          'spleen_high'\n         ]]\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y , test_size=0.2, stratify=y, random_state=101\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train ,y_train , test_size=0.1, stratify=y_train, random_state=101\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:05:39.469583Z","iopub.execute_input":"2023-11-07T00:05:39.470435Z","iopub.status.idle":"2023-11-07T00:05:39.530483Z","shell.execute_reply.started":"2023-11-07T00:05:39.470401Z","shell.execute_reply":"2023-11-07T00:05:39.529557Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"train = df_total[df_total['serial_id'].isin(X_train)]\ntest = df_total[df_total['serial_id'].isin(X_test)]\nval = df_total[df_total['serial_id'].isin(X_val)]","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:05:45.421151Z","iopub.execute_input":"2023-11-07T00:05:45.421514Z","iopub.status.idle":"2023-11-07T00:05:45.452120Z","shell.execute_reply.started":"2023-11-07T00:05:45.421483Z","shell.execute_reply":"2023-11-07T00:05:45.451340Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def create_sequences_with_step_image(data: pd.DataFrame, seq_len, step):\n    sequences = []\n    data_size = len(data)\n\n    for i in tqdm(range(0, data_size - seq_len + 1, step)):\n        feature = data.iloc[i: i + seq_len,3]\n        label_position = (i+seq_len-1)\n        label = data.iloc[label_position,5:-1]\n        \n        sequences.append([feature,label])\n        \n    return sequences\n\nseq_len = num_slices","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:05:49.000133Z","iopub.execute_input":"2023-11-07T00:05:49.000941Z","iopub.status.idle":"2023-11-07T00:05:49.007403Z","shell.execute_reply.started":"2023-11-07T00:05:49.000910Z","shell.execute_reply":"2023-11-07T00:05:49.006386Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"train_s = create_sequences_with_step_image(train,seq_len,seq_len)\ntest_s = create_sequences_with_step_image(test,seq_len,seq_len)\nval_s = create_sequences_with_step_image(val,seq_len,seq_len)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:05:51.680795Z","iopub.execute_input":"2023-11-07T00:05:51.681520Z","iopub.status.idle":"2023-11-07T00:05:52.369371Z","shell.execute_reply.started":"2023-11-07T00:05:51.681486Z","shell.execute_reply":"2023-11-07T00:05:52.368542Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"100%|██████████| 927/927 [00:00<00:00, 1579.41it/s]\n100%|██████████| 258/258 [00:00<00:00, 4130.46it/s]\n100%|██████████| 103/103 [00:00<00:00, 4221.11it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"num_slices=50\ntest = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/test_series_meta.csv')\ntest_pat_id = test['patient_id'].unique()\n\n# Define the root directory where the patient data is located\nroot_dir = '/kaggle/input/rsna-2023-abdominal-trauma-detection/test_images'\n\n# Define the list of patient IDs you want to use (replace with your selected IDs)\nselected_patient_ids = test_pat_id \n\n# Initialize empty lists to store data\npatient_ids = []\nimage_paths = []\n\n# Traverse the directories and collect the data\nfor patient_id in selected_patient_ids:\n    patient_dir = os.path.join(root_dir, str(patient_id))\n    if os.path.exists(patient_dir):\n        for serial_id in os.listdir(patient_dir):\n            serial_dir = os.path.join(patient_dir, serial_id)\n            if os.path.isdir(serial_dir):\n                # List all .dcm files in the serial_id folder\n                image_files = [filename for filename in os.listdir(serial_dir) if filename.endswith('.dcm')]\n                \n                # Calculate the step size for evenly spaced selection\n                total_images = len(image_files)\n                if total_images <= num_slices:\n                    step_size = 1  # If fewer than num_slices images, select all of them\n                else:\n                    step_size = total_images // num_slices\n                \n                # Select images in an evenly spaced manner\n                selected_images = [image_files[i] for i in range(0, total_images, step_size)][:num_slices]\n                \n                for filename in selected_images:\n                    image_path = os.path.join(serial_dir, filename)\n                    patient_ids.append(patient_id)\n                    image_paths.append(image_path)\n\n# Create a DataFrame from the collected data\ndf_test = {'patient_id': patient_ids, 'image_path': image_paths}\ndf_test = pd.DataFrame(df_test)\ndf_test_num_slices = pd.DataFrame()\nfor i in df_test.index:\n    df= pd.concat([df_test.iloc[i:i+1,:]]*num_slices,axis=0, ignore_index=True)\n    df_test_num_slices = pd.concat([df_test_num_slices,df],axis=0, ignore_index=True)\ndef create_sequences_with_step_image_inf(data: pd.DataFrame, seq_len, step):\n    sequences = []\n    data_size = len(data)\n\n    for i in tqdm(range(0, data_size - seq_len + 1, step)):\n        feature = data.iloc[i: i + seq_len,1]\n        sequences.append([feature])\n    return sequences\n\nseq_len = num_slices\ntest_seq = create_sequences_with_step_image_inf(df_test_num_slices,seq_len,seq_len)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T16:50:32.824712Z","iopub.execute_input":"2023-11-06T16:50:32.825106Z","iopub.status.idle":"2023-11-06T16:50:32.875101Z","shell.execute_reply.started":"2023-11-06T16:50:32.825078Z","shell.execute_reply":"2023-11-06T16:50:32.874112Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 6403.52it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"np.save('train_s.npy', train_s)\nnp.save('test_s.npy', train_s)\nnp.save('val_s.npy', train_s)\nnp.save('test_seq.npy', test_seq)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T02:29:52.938193Z","iopub.execute_input":"2023-11-06T02:29:52.939124Z","iopub.status.idle":"2023-11-06T02:29:53.443341Z","shell.execute_reply.started":"2023-11-06T02:29:52.939078Z","shell.execute_reply":"2023-11-06T02:29:53.442395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_s = np.load('/kaggle/input/csvfile/train_s.npy')\ntest_s = ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SeqToImageDataset3D(Dataset):\n\n    def __init__(self,sequences,\n                ):\n        self.sequences = sequences\n    def __len__(self):\n        return len(self.sequences)\n    def __getitem__(self, idx):   \n        sequence, labels = self.sequences[idx]\n        imgs = []\n        mean = [0.485]*50\n        std = [0.225]*50\n        for image_path in sequence:\n            path = image_path\n            ds = pydicom.dcmread(path)\n            image = ds.pixel_array\n            image = Image.fromarray(image).convert('L')\n            image = image.resize((224,224))\n            array = np.array(image)\n            imgs.append(array)\n        imgs = np.array(imgs)\n        t = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize(mean=mean, std=std)])\n        tspd = imgs.transpose(1,2,0)\n        imgs = t(tspd)\n        return {'sequence': imgs, 'labels': torch.tensor(labels, dtype=torch.float32)}","metadata":{"execution":{"iopub.status.busy":"2023-11-06T16:50:48.134706Z","iopub.execute_input":"2023-11-06T16:50:48.135403Z","iopub.status.idle":"2023-11-06T16:50:48.143958Z","shell.execute_reply.started":"2023-11-06T16:50:48.135371Z","shell.execute_reply":"2023-11-06T16:50:48.142825Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class SeqToImageDataset3DINF(Dataset):\n\n    def __init__(self,sequences,\n                ):\n        self.sequences = sequences\n    def __len__(self):\n        return len(self.sequences)\n    def __getitem__(self, idx):   \n        sequence = self.sequences[idx]\n        imgs = []\n        mean = [0.485]*50\n        std = [0.225]*50\n        for image_path in sequence[0]:\n            path = image_path\n            ds = pydicom.dcmread(path)\n            image = ds.pixel_array\n            image = Image.fromarray(image).convert('L')\n            image = image.resize((224,224))\n            array = np.array(image)\n            imgs.append(array)\n        imgs = np.array(imgs)\n        t = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize(mean=mean, std=std)])\n        tspd = imgs.transpose(1,2,0)\n        imgs = t(tspd)\n        return imgs","metadata":{"execution":{"iopub.status.busy":"2023-11-06T16:50:48.420951Z","iopub.execute_input":"2023-11-06T16:50:48.421736Z","iopub.status.idle":"2023-11-06T16:50:48.430841Z","shell.execute_reply.started":"2023-11-06T16:50:48.421704Z","shell.execute_reply":"2023-11-06T16:50:48.429644Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class CustomDataModule3D(L.LightningDataModule):\n    def __init__(self, train_dataframe, val_dataframe, test_dataframe, \n                 pred_dataframe, \n                 batch_size=config.BATCH_SIZE):\n        super().__init__()\n        self.train_dataframe = train_dataframe\n        self.val_dataframe = val_dataframe\n        self.test_dataframe = test_dataframe\n        self.pred_dataframe = pred_dataframe\n\n        self.batch_size = batch_size\n        \n        random.seed(config.SEED)\n        np.random.seed(config.SEED)\n        torch.manual_seed(config.SEED)\n\n    def setup(self, stage=None):\n        self.train_dataset = SeqToImageDataset3D(self.train_dataframe)\n        self.val_dataset = SeqToImageDataset3D(self.val_dataframe)\n        self.test_dataset = SeqToImageDataset3D(self.test_dataframe)\n        self.pred_dataset = SeqToImageDataset3DINF(self.pred_dataframe)\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=1\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.val_dataset,\n            batch_size=len(self.val_dataframe)//5,\n            shuffle=False,\n            num_workers=1\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.test_dataset,\n            batch_size=len(self.test_dataframe)//5,\n            shuffle=False,\n            num_workers=1\n        )\n\n    def predict_dataloader(self):\n        return DataLoader(\n            dataset=self.pred_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=1\n        )","metadata":{"execution":{"iopub.status.busy":"2023-11-06T16:51:29.358338Z","iopub.execute_input":"2023-11-06T16:51:29.358801Z","iopub.status.idle":"2023-11-06T16:51:29.369089Z","shell.execute_reply.started":"2023-11-06T16:51:29.358772Z","shell.execute_reply":"2023-11-06T16:51:29.368045Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class ResNetImage(nn.Module):\n    \n    def __init__(self):\n        super(ResNetImage, self).__init__()\n        \n        self.features = torchvision.models.resnet18(weights='DEFAULT')\n        \n        self.features.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=num_slices, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True))\n        \n        self.features.fc = nn.Linear(in_features=512, out_features=1000, bias = True)\n        \n        self.layersB = nn.Sequential(\n            nn.BatchNorm1d(1000),\n            nn.ReLU(),\n            nn.Linear(in_features=1000, out_features=2, bias=True),\n        )\n        \n        self.layersE = nn.Sequential(\n            nn.BatchNorm1d(1000),\n            nn.ReLU(),\n            nn.Linear(in_features=1000, out_features=2, bias=True),\n        )\n        \n        \n        self.layersK = nn.Sequential(\n            nn.BatchNorm1d(1000),\n            nn.ReLU(),\n            nn.Linear(in_features=1000, out_features=3, bias=True),\n        )\n        \n        \n        self.layersL = nn.Sequential(\n            nn.BatchNorm1d(1000),\n            nn.ReLU(),\n            nn.Linear(in_features=1000, out_features=3, bias=True),\n        )\n  \n        \n        self.layersS = nn.Sequential(\n            nn.BatchNorm1d(1000),\n            nn.ReLU(),\n            nn.Linear(in_features=1000, out_features=3, bias=True),\n        )\n    \n    def forward(self, x):       \n        out = self.features(x)  # Pass each image through the feature extractor\n        \n        # Perform classification for each task\n        bowel_logits = self.layersB(out)\n        extravasation_logits = self.layersE(out)\n        kidney_logits = self.layersK(out)\n        liver_logits = self.layersL(out)\n        spleen_logits = self.layersS(out)\n        \n        return {\n            'bowel': bowel_logits,\n            'extravasation': extravasation_logits,\n            'kidney': kidney_logits,\n            'liver': liver_logits,\n            'spleen': spleen_logits\n        }","metadata":{"execution":{"iopub.status.busy":"2023-11-06T16:51:37.319938Z","iopub.execute_input":"2023-11-06T16:51:37.320279Z","iopub.status.idle":"2023-11-06T16:51:37.332486Z","shell.execute_reply.started":"2023-11-06T16:51:37.320254Z","shell.execute_reply":"2023-11-06T16:51:37.331526Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\n#Promissor e funcionando\nclass LightM(L.LightningModule):\n    def __init__(self, model, lr=config.LR):\n        super(LightM, self).__init__()\n        self.lr = lr\n        self.model = model\n        \n        self.loss_fn1 = nn.BCEWithLogitsLoss()\n        self.loss_fn2 = nn.BCEWithLogitsLoss()\n        self.loss_fn3 = nn.CrossEntropyLoss()\n        self.loss_fn4 = nn.CrossEntropyLoss()\n        self.loss_fn5 = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        inputs, labels = batch['sequence'], batch['labels']\n\n        # Compute logits for both tasks\n        logits = self(inputs)\n        bowel_logits = logits['bowel']\n        extravasation_logits = logits[\"extravasation\"]\n        kidney_logits = logits['kidney']\n        liver_logits = logits['liver']\n        spleen_logits = logits['spleen']\n\n        # Compute losses and accuracies for both tasks\n        bowel_loss = self.loss_fn1(bowel_logits, labels[:,0:2])\n        extravasation_loss = self.loss_fn2(extravasation_logits, labels[:,2:4])\n        kidney_loss = self.loss_fn3(kidney_logits, labels[:,4:7])\n        liver_loss = self.loss_fn4(liver_logits, labels[:,7:10])\n        spleen_loss = self.loss_fn5(spleen_logits, labels[:,10:])\n        \n        # Compute the total loss (you can weigh the losses if needed)\n        total_loss = bowel_loss + extravasation_loss + kidney_loss + liver_loss + spleen_loss\n\n        self.log('train_bowel_loss', bowel_loss)\n        self.log('train_extravasation_loss', extravasation_loss)\n        self.log('train_kidney_loss', kidney_loss)\n        self.log('train_liver_loss', liver_loss)\n        self.log('train_spleen_loss',spleen_loss)\n        self.log('train_loss',total_loss, prog_bar=True, logger=True)\n\n        #preds\n        bowel_pred = torch.argmax(bowel_logits, dim=1)\n        extravasation_pred = torch.argmax(extravasation_logits, dim=1)\n        kidney_pred = torch.argmax(kidney_logits, dim=1) \n        liver_pred = torch.argmax(liver_logits, dim=1)\n        spleen_pred = torch.argmax(spleen_logits, dim=1)\n        #total_pred = torch.cat([bowel_pred, extravasation_pred, kidney_pred, liver_pred, spleen_pred], dim=1)\n        acc1 =torchmetrics.Accuracy(task='binary').to(device)\n        acc2 =torchmetrics.Accuracy(task='binary').to(device)\n        acc3 =torchmetrics.Accuracy(task='multiclass', num_classes=3).to(device)\n        acc4 =torchmetrics.Accuracy(task='multiclass', num_classes=3).to(device)\n        acc5 =torchmetrics.Accuracy(task='multiclass', num_classes=3).to(device)\n        accuracy1 = acc1(bowel_pred,torch.argmax(labels[:,0:2], dim=1))\n        accuracy2 = acc2(extravasation_pred,torch.argmax(labels[:,2:4], dim=1))\n        accuracy3 = acc3(kidney_pred,torch.argmax(labels[:,4:7], dim=1))\n        accuracy4 = acc4(liver_pred,torch.argmax(labels[:,7:10], dim=1))\n        accuracy5 = acc5(spleen_pred,torch.argmax(labels[:,10:], dim=1))\n        acc = (accuracy1 + accuracy2 + accuracy3 + accuracy4 + accuracy5)/5\n        self.log('train_bowel_acc', accuracy1)\n        self.log('train_extravasation_acc', accuracy2)\n        self.log('train_kidney_acc', accuracy3)\n        self.log('train_liver_acc', accuracy4)\n        self.log('train_spleen_acc',accuracy5)\n        self.log('train_acc',acc, prog_bar=True, logger=True)\n\n        return total_loss\n    \n    def validation_step(self, batch, batch_idx):\n        inputs, labels = batch['sequence'], batch['labels']\n\n        # Compute logits for both tasks\n        logits = self(inputs)\n        bowel_logits = logits['bowel']\n        extravasation_logits = logits[\"extravasation\"]\n        kidney_logits = logits['kidney']\n        liver_logits = logits['liver']\n        spleen_logits = logits['spleen']\n\n        # Compute losses and accuracies for both tasks\n        bowel_loss = self.loss_fn1(bowel_logits, labels[:,0:2])\n        extravasation_loss = self.loss_fn2(extravasation_logits, labels[:,2:4])\n        kidney_loss = self.loss_fn3(kidney_logits, labels[:,4:7])\n        liver_loss = self.loss_fn4(liver_logits, labels[:,7:10])\n        spleen_loss = self.loss_fn5(spleen_logits, labels[:,10:])\n\n\n        # Compute the total loss (you can weigh the losses if needed)\n        total_loss = bowel_loss + extravasation_loss + kidney_loss + liver_loss + spleen_loss\n\n        self.log('val_bowel_loss', bowel_loss)\n        self.log('val_extravasation_loss', extravasation_loss)\n        self.log('val_kidney_loss', kidney_loss)\n        self.log('val_liver_loss', liver_loss)\n        self.log('val_spleen_loss',spleen_loss)\n        self.log('val_loss',total_loss, prog_bar=True, logger=True)\n        \n        #preds\n        bowel_pred = torch.argmax(bowel_logits, dim=1)\n        extravasation_pred = torch.argmax(extravasation_logits, dim=1)\n        kidney_pred = torch.argmax(kidney_logits, dim=1) \n        liver_pred = torch.argmax(liver_logits, dim=1)\n        spleen_pred = torch.argmax(spleen_logits, dim=1)\n        #total_pred = torch.cat([bowel_pred, extravasation_pred, kidney_pred, liver_pred, spleen_pred], dim=1)\n        acc1 =torchmetrics.Accuracy(task='binary').to(device)\n        acc2 =torchmetrics.Accuracy(task='binary').to(device)\n        acc3 =torchmetrics.Accuracy(task='multiclass', num_classes=3).to(device)\n        acc4 =torchmetrics.Accuracy(task='multiclass', num_classes=3).to(device)\n        acc5 =torchmetrics.Accuracy(task='multiclass', num_classes=3).to(device)\n        accuracy1 = acc1(bowel_pred,torch.argmax(labels[:,0:2], dim=1))\n        accuracy2 = acc2(extravasation_pred,torch.argmax(labels[:,2:4], dim=1))\n        accuracy3 = acc3(kidney_pred,torch.argmax(labels[:,4:7], dim=1))\n        accuracy4 = acc4(liver_pred,torch.argmax(labels[:,7:10], dim=1))\n        accuracy5 = acc5(spleen_pred,torch.argmax(labels[:,10:], dim=1))\n        acc = (accuracy1 + accuracy2 + accuracy3 + accuracy4 + accuracy5)/5\n        self.log('val_bowel_acc', accuracy1)\n        self.log('val_extravasation_acc', accuracy2)\n        self.log('val_kidney_acc', accuracy3)\n        self.log('val_liver_acc', accuracy4)\n        self.log('val_spleen_acc',accuracy5)\n        self.log('val_acc',acc, prog_bar=True, logger=True)\n        \n        return total_loss\n    \n    def test_step(self, batch, batch_idx):\n        inputs, labels = batch['sequence'], batch['labels']\n\n        # Compute logits for both tasks\n        logits = self(inputs)\n        bowel_logits = logits['bowel']\n        extravasation_logits = logits[\"extravasation\"]\n        kidney_logits = logits['kidney']\n        liver_logits = logits['liver']\n        spleen_logits = logits['spleen']\n\n        # Compute losses and accuracies for both tasks\n        bowel_loss = self.loss_fn1(bowel_logits, labels[:,0:2])\n        extravasation_loss = self.loss_fn2(extravasation_logits, labels[:,2:4])\n        kidney_loss = self.loss_fn3(kidney_logits, labels[:,4:7])\n        liver_loss = self.loss_fn4(liver_logits, labels[:,7:10])\n        spleen_loss = self.loss_fn5(spleen_logits, labels[:,10:])\n\n\n        # Compute the total loss (you can weigh the losses if needed)\n        total_loss = bowel_loss + extravasation_loss + kidney_loss + liver_loss + spleen_loss\n\n        self.log('test_bowel_loss', bowel_loss)\n        self.log('test_extravasation_loss', extravasation_loss)\n        self.log('test_kidney_loss', kidney_loss)\n        self.log('test_liver_loss', liver_loss)\n        self.log('test_spleen_loss',spleen_loss)\n        self.log('test_loss',total_loss, prog_bar=True, logger=True)\n        \n        #preds\n        bowel_pred = torch.argmax(bowel_logits, dim=1)\n        extravasation_pred = torch.argmax(extravasation_logits, dim=1)\n        kidney_pred = torch.argmax(kidney_logits, dim=1) \n        liver_pred = torch.argmax(liver_logits, dim=1)\n        spleen_pred = torch.argmax(spleen_logits, dim=1)\n        #total_pred = torch.cat([bowel_pred, extravasation_pred, kidney_pred, liver_pred, spleen_pred], dim=1)\n        acc1 =torchmetrics.Accuracy(task='binary').to(device)\n        acc2 =torchmetrics.Accuracy(task='binary').to(device)\n        acc3 =torchmetrics.Accuracy(task='multiclass', num_classes=3).to(device)\n        acc4 =torchmetrics.Accuracy(task='multiclass', num_classes=3).to(device)\n        acc5 =torchmetrics.Accuracy(task='multiclass', num_classes=3).to(device)\n        accuracy1 = acc1(bowel_pred,torch.argmax(labels[:,0:2], dim=1))\n        accuracy2 = acc2(extravasation_pred,torch.argmax(labels[:,2:4], dim=1))\n        accuracy3 = acc3(kidney_pred,torch.argmax(labels[:,4:7], dim=1))\n        accuracy4 = acc4(liver_pred,torch.argmax(labels[:,7:10], dim=1))\n        accuracy5 = acc5(spleen_pred,torch.argmax(labels[:,10:], dim=1))\n        acc = (accuracy1 + accuracy2 + accuracy3 + accuracy4 + accuracy5)/5\n        self.log('test_bowel_acc', accuracy1)\n        self.log('test_extravasation_acc', accuracy2)\n        self.log('test_kidney_acc', accuracy3)\n        self.log('test_liver_acc', accuracy4)\n        self.log('test_spleen_acc',accuracy5)\n        self.log('test_acc',acc, prog_bar=True, logger=True)\n        \n        return total_loss\n    \n    def prediction_step(self, batch, batch_idx):\n        inputs = batch\n\n        logits = self(inputs)\n\n        return logits\n\n    def configure_optimizers(self):\n        opt = torch.optim.AdamW(self.parameters(), lr=self.lr)\n        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=57, eta_min = 0.0005) # New!\n\n        return {\n            \"optimizer\": opt,\n            \"lr_scheduler\": {\n                \"scheduler\": sch,\n                \"monitor\": \"train_loss\",\n                \"interval\": \"step\",\n                \"frequency\": 1\n            },\n        }","metadata":{"execution":{"iopub.status.busy":"2023-11-06T16:51:39.951231Z","iopub.execute_input":"2023-11-06T16:51:39.951548Z","iopub.status.idle":"2023-11-06T16:51:39.994021Z","shell.execute_reply.started":"2023-11-06T16:51:39.951524Z","shell.execute_reply":"2023-11-06T16:51:39.993063Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#train_s = np.load('/kaggle/input/csvfile/train_s.npy', allow_pickle=True)\n#test_s = np.load('/kaggle/input/csvfile/test_s.npy', allow_pickle=True)\n#test_seq = np.load('/kaggle/input/csvfile/test_seq.npy', allow_pickle=True)\n#val_s = np.load('/kaggle/input/csvfile/val_s.npy', allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T17:00:12.821796Z","iopub.execute_input":"2023-11-06T17:00:12.822243Z","iopub.status.idle":"2023-11-06T17:00:14.174573Z","shell.execute_reply.started":"2023-11-06T17:00:12.822213Z","shell.execute_reply":"2023-11-06T17:00:14.173522Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"module = ResNetImage()\nmodel = LightM(model=module, lr=config.LR)\n\ndm = CustomDataModule3D(train_s,\n                         val_s,\n                         test_s,\n                         test_seq,\n                         batch_size=config.BATCH_SIZE,\n                        )\n\nlogger_v2 = TensorBoardLogger(save_dir=\"/kaggle/working/\", name=\"logs_v2\")\ncallback_v2 = [ModelCheckpoint(save_top_k=1,\n                             verbose=True,\n                             monitor='val_loss',\n                             save_last = True,\n                             mode='min',\n                             filename='best_model',\n                             dirpath = logger_v2.log_dir\n                              ),\n               EarlyStopping(monitor='val_loss',\n                             patience=3,\n                             mode='min',\n                            )\n              ]\n\ntrainer = L.Trainer(\n    max_epochs=config.MAX_EPOCHS,\n    callbacks = callback_v2,\n    accelerator=\"gpu\",\n    logger=logger_v2,\n    deterministic=True,\n    precision='16-mixed',\n    detect_anomaly=True,\n    log_every_n_steps=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T17:00:24.302180Z","iopub.execute_input":"2023-11-06T17:00:24.303066Z","iopub.status.idle":"2023-11-06T17:00:24.921590Z","shell.execute_reply.started":"2023-11-06T17:00:24.303032Z","shell.execute_reply":"2023-11-06T17:00:24.920573Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model,datamodule=dm)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T02:31:38.721151Z","iopub.execute_input":"2023-11-06T02:31:38.721834Z","iopub.status.idle":"2023-11-06T02:51:03.557340Z","shell.execute_reply.started":"2023-11-06T02:31:38.721798Z","shell.execute_reply":"2023-11-06T02:51:03.556105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.test(datamodule=dm, ckpt_path='best', verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T00:48:28.062508Z","iopub.status.idle":"2023-11-05T00:48:28.062902Z","shell.execute_reply.started":"2023-11-05T00:48:28.062723Z","shell.execute_reply":"2023-11-05T00:48:28.062743Z"},"trusted":true},"execution_count":null,"outputs":[]}]}